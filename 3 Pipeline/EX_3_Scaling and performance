###############################################################################################################
Part 1
Rewrite the example to use nlp.pipe. Instead of iterating over the texts and processing them, iterate over the doc objects yielded by nlp.pipe.
###############################################################################################################

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
nlp = spacy.load("en_core_web_sm")

with open("exercises/en/tweets.json") as f:
    TEXTS = json.loads(f.read())

# Process the texts and print the adjectives
def get_adjectives(doc_obj):
  adj_ls=[x.text for x in doc_obj if x.pos_ == "ADJ"]
  #return (doc_obj,adj_ls)
  #print(adj_ls)
  #return doc_obj
  return adj_ls

#nlp.add_pipe(get_adjectives,last=True)
#Doc.set_extension("get_adj", method=get_adjectives)
doc=list(nlp.pipe(TEXTS))
#print(type(doc[0]),doc[0])
#print(doc[0].ents)
#print(doc[0].to_json())
#print(doc[0].pos_)
#[print(x.pos_) for x in doc[0]]
print([get_adjectives(x) for x in doc])
#print([.text for full_doc in doc if full_doc.ents.pos_ == "ADJ"])
#for text in TEXTS:
#    #doc = nlp(text)
#    doc=list(nlp.pipe(text))
#    print([token.text for token in doc if token.pos_ == "ADJ"])
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
import json
import spacy

nlp = spacy.load("en_core_web_sm")

with open("exercises/en/tweets.json") as f:
    TEXTS = json.loads(f.read())

# Process the texts and print the adjectives
#doc=list(nlp.pipe(TEXTS))
#for doc_f in doc:
for doc_f in nlp.pipe(TEXTS):
  print([token.text for token in doc_f if token.pos_ == "ADJ"])
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

###############################################################################################################
Part 2
Rewrite the example to use nlp.pipe. Don’t forget to call list() around the result to turn it into a list.
###############################################################################################################
import json
import spacy

nlp = spacy.load("en_core_web_sm")

with open("exercises/en/tweets.json") as f:
    TEXTS = json.loads(f.read())

# Process the texts and print the entities
#docs = [nlp(text) for text in TEXTS]
docs = list(nlp.pipe(TEXTS))
entities = [doc.ents for doc in docs]
print(*entities)


###############################################################################################################
Part 3
Rewrite the example to use nlp.pipe. Don’t forget to call list() around the result to turn it into a list.
###############################################################################################################
from spacy.lang.en import English

nlp = English()

people = ["David Bowie", "Angela Merkel", "Lady Gaga"]

# Create a list of patterns for the PhraseMatcher
patterns = list(nlp.pipe(people))

###############################################################################################################
PROCESSING DATA WITH CONTEXT
Use the set_extension method to register the custom attributes "author" and "book" on the Doc, which default to None.
Process the [text, context] pairs in DATA using nlp.pipe with as_tuples=True.
Overwrite the doc._.book and doc._.author with the respective info passed in as the context.
###############################################################################################################
import json
from spacy.lang.en import English
from spacy.tokens import Doc

with open("exercises/en/bookquotes.json") as f:
    DATA = json.loads(f.read())

nlp = English()

# Register the Doc extension "author" (default None)
Doc.set_extension("author",default=None)

# Register the Doc extension "book" (default None)
Doc.set_extension("book",default=None)

for doc, context in nlp.pipe(DATA, as_tuples=True):
    # Set the doc._.book and doc._.author attributes from the context
    print(context)
    doc._.book = context["book"]
    doc._.author = context["author"]

    # Print the text and custom attribute data
    print(f"{doc.text}\n — '{doc._.book}' by {doc._.author}\n")

###############################################################################################################
SELECTIVE PROCESSING
Rewrite the code to only tokenize the text using nlp.make_doc
###############################################################################################################
import spacy

nlp = spacy.load("en_core_web_sm")
text = (
    "Chick-fil-A is an American fast food restaurant chain headquartered in "
    "the city of College Park, Georgia, specializing in chicken sandwiches."
)

# Only tokenize the text
#doc = nlp(text)
doc = nlp.make_doc(text)
print([token.text for token in doc])
###############################################################################################################
Part 2
Disable the tagger and parser using the nlp.disable_pipes method.
Process the text and print all entities in the doc.
###############################################################################################################
import spacy

nlp = spacy.load("en_core_web_sm")
text = (
    "Chick-fil-A is an American fast food restaurant chain headquartered in "
    "the city of College Park, Georgia, specializing in chicken sandwiches."
)

# Disable the tagger and parser
with nlp.disable_pipes("tagger", "parser"):
    # Process the text
    doc = nlp(text)
    # Print the entities in the doc
    print(doc.ents)

###############################################################################################################

