Word vectors and semantic similarity
Comparing semantic similarity
	spaCy can compare two objects and predict similarity
	Doc.similarity(), Span.similarity() and Token.similarity()
	Take another object and return a similarity score (0 to 1)
	Important: needs a model that has word vectors included, for example:
		âœ… en_core_web_md (medium model)
		âœ… en_core_web_lg (large model)
		ðŸš« NOT en_core_web_sm (small model)

Similarity examples (1)
# Load a larger model with vectors
nlp = spacy.load("en_core_web_md")

# Compare two documents
	doc1 = nlp("I like fast food")
	doc2 = nlp("I like pizza")
	print(doc1.similarity(doc2))
	0.8627204117787385
# Compare two tokens
	doc = nlp("I like pizza and pasta")
	token1 = doc[2]
	token2 = doc[4]
	print(token1.similarity(token2))
	0.7369546
# Compare a document with a token
	doc = nlp("I like pizza")
	token = nlp("soap")[0]

	print(doc.similarity(token))
# Compare a span with a document
	span = nlp("I like pizza and pasta")[2:5]
	doc = nlp("McDonalds sells burgers")

	print(span.similarity(doc))
	
How does spaCy predict similarity?
	Similarity is determined using word vectors
	Multi-dimensional meaning representations of words
	Generated using an algorithm like Word2Vec and lots of text
	Can be added to spaCy's statistical models
	Default: cosine similarity, but can be adjusted
	Doc and Span vectors default to average of token vectors
	Short phrases are better than long documents with many irrelevant words


Word vectors in spaCy
	# Load a larger model with vectors
	nlp = spacy.load("en_core_web_md")

	doc = nlp("I have a banana")
	# Access the vector via the token.vector attribute
	print(doc[3].vector)
	
Similarity depends on the application context
	Useful for many applications: recommendation systems, flagging duplicates etc.
	There's no objective definition of "similarity"
	Depends on the context and what application needs to do
		doc1 = nlp("I like cats")
		doc2 = nlp("I hate cats")

		print(doc1.similarity(doc2))